{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf0d686-1606-429a-9de2-bd5d4443aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "No clouds file shape: (113845, 50)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def parse_json_column(column):\n",
    "    \"\"\"\n",
    "    Safely parse a column that contains JSON-like strings or dictionaries.\n",
    "    If the column is empty, null, or invalid, return an empty dictionary.\n",
    "    This function handles cases where the column might already be a dictionary or a JSON string.\n",
    "    \"\"\"\n",
    "    # Check if the column is empty, null, or already an empty dictionary/list\n",
    "    if pd.isna(column) or column == {} or column == []:\n",
    "        return {}\n",
    "    # If the column is already a dictionary, return it as is\n",
    "    if isinstance(column, dict):\n",
    "        return column\n",
    "    # If the column is a string, try to parse it as JSON\n",
    "    if isinstance(column, str):\n",
    "        try:\n",
    "            # Replace single quotes with double quotes to ensure valid JSON format\n",
    "            column = column.replace(\"'\", '\"')\n",
    "            # Unescape any escaped characters in the string\n",
    "            column = bytes(column, \"utf-8\").decode(\"unicode_escape\")\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            return json.loads(column)\n",
    "        except (ValueError, SyntaxError, json.JSONDecodeError):\n",
    "            # If parsing fails, return an empty dictionary\n",
    "            return {}\n",
    "def flatten_and_merge(df, column, prefix):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame, parses a specified JSON column, flattens it into multiple columns,\n",
    "    and merges the flattened data back into the original DataFrame.\n",
    "    The flattened columns are prefixed with the given prefix for clarity.\n",
    "    \"\"\"\n",
    "    # Create a new column name for the parsed JSON data\n",
    "    parsed_column = f\"{column}_parsed\"\n",
    "    # Parse the JSON column using the parse_json_column function\n",
    "    df[parsed_column] = df[column].apply(parse_json_column)\n",
    "    # Flatten the parsed JSON column into separate columns\n",
    "    flattened_df = pd.json_normalize(df[parsed_column])\n",
    "    # Add a prefix to the flattened columns to avoid naming conflicts\n",
    "    flattened_df = flattened_df.add_prefix(f\"{prefix}.\")\n",
    "    # Reset the indices of both DataFrames to ensure proper alignment during concatenation\n",
    "    flattened_df = flattened_df.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Merge the flattened DataFrame back into the original DataFrame\n",
    "    df = pd.concat([df, flattened_df], axis=1)\n",
    "    # Drop the intermediate parsed column as it's no longer needed\n",
    "    df = df.drop(columns=[parsed_column])\n",
    "    return df\n",
    "def explode_and_flatten(df, column, prefix):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame, explodes a specified column (which contains lists or JSON-like structures),\n",
    "    flattens the exploded values into separate columns, and merges the result back into the original DataFrame.\n",
    "    The flattened columns are prefixed with the given prefix for clarity.\n",
    "    \"\"\"\n",
    "    # Explode the column to create one row for each element in the list/JSON structure\n",
    "    df = df.explode(column)\n",
    "    # Reset the index after exploding to ensure all indices are unique and aligned\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Flatten the exploded column into separate columns\n",
    "    flattened_df = pd.json_normalize(df[column])\n",
    "    # Add a prefix to the flattened columns to avoid naming conflicts\n",
    "    flattened_df = flattened_df.add_prefix(f\"{prefix}.\")\n",
    "    # Reset the index of the flattened DataFrame to ensure proper alignment\n",
    "    flattened_df = flattened_df.reset_index(drop=True)\n",
    "    # Merge the flattened DataFrame back into the original DataFrame\n",
    "    df = pd.concat([df, flattened_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Execution part\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:/Users/lesze/classifications.csv', sep=',') #file with Zooniverse classifications\n",
    "df = df.drop(columns=['gold_standard','expert'])\n",
    "print('data loaded')\n",
    "\n",
    "df = df[(df.workflow_name == 'Are Clouds Present?')]\n",
    "df['created_at'] = pd.to_datetime(df['created_at']).dt.tz_localize(None)\n",
    "df = df[(df.created_at > '2024-10-01')]\n",
    "\n",
    "# Process the 'metadata' column: parse, flatten, and merge it back into the DataFrame\n",
    "df = flatten_and_merge(df, 'metadata', 'metadata')\n",
    "\n",
    "df['metadata.subject_dimensions']= df['metadata.subject_dimensions'].str.get(0)\n",
    "\n",
    "df[['metadata.subject_dimensions.clientWidth',\\\n",
    "    'metadata.subject_dimensions.clientHeight',\\\n",
    "    'metadata.subject_dimensions.naturalWidth',\\\n",
    "    'metadata.subject_dimensions.naturalHeight']]\\\n",
    "       = df['metadata.subject_dimensions']\\\n",
    "       .astype(str)\\\n",
    "       .str.replace(\"{'clientWidth':\",\"\")\\\n",
    "       .str.replace(\" 'clientHeight': \",\"\")\\\n",
    "       .str.replace(\" 'naturalWidth': \",\"\")\\\n",
    "       .str.replace(\" 'naturalHeight': \",\"\")\\\n",
    "       .str.replace(\"}\",\"\")\\\n",
    "       .str.split(\",\", expand=True)\n",
    "\n",
    "df['subject_data.orbit'] = df['subject_data'].astype(str)\\\n",
    "              .str.split('\"Image_Orbit\":\"', expand=True)[1]\\\n",
    "              .str.split('\"',expand=True)[0]\n",
    "df['subject_data.Ls'] = df['subject_data'].astype(str)\\\n",
    "              .str.split('\"Angle\":\"', expand=True)[1]\\\n",
    "              .str.split('\"',expand=True)[0]              \n",
    "df['subject_data.Angle'] = df['subject_data'].astype(str)\\\n",
    "              .str.split('\"Angle\":\"', expand=True)[1]\\\n",
    "              .str.split('\"',expand=True)[0]\n",
    "df['subject_data.Binning'] = df['subject_data'].astype(str)\\\n",
    "              .str.split('\"Binning\":\"', expand=True)[1]\\\n",
    "              .str.split('\"',expand=True)[0]   \n",
    "\n",
    "\n",
    "df['annotations_0'] = df['annotations'].apply(json.loads).apply(pd.Series)[0]\n",
    "df = flatten_and_merge(df, 'annotations_0', 'annotations_0')\n",
    "df['annotations_1'] = df['annotations'].apply(json.loads).apply(pd.Series)[1]\n",
    "df = flatten_and_merge(df, 'annotations_1', 'annotations_1')\n",
    "\n",
    "# Explode and flatten the 'annotations.1.value' column\n",
    "df = explode_and_flatten(df, 'annotations_1.value', 'annotations_1.value')\n",
    "\n",
    "df['orbit+username'] = df['subject_data.orbit'].astype(str) + '+' + df['user_name'].astype(str)\n",
    "df['user_orbit_counts'] = df.groupby('orbit+username')['orbit+username'].transform('count')\n",
    "df['rank'] = np.where(df['user_orbit_counts'] < 20, 'student',\n",
    "             np.where(df['user_orbit_counts'] < 100, 'advanced', 'elite'))\n",
    "\n",
    "df['weight'] = np.where(df['user_orbit_counts'] < 20, 1,\n",
    "              np.where(df['user_orbit_counts'] < 100, 2, 3))\n",
    "\n",
    "#unnecassary columns drop\n",
    "df = df.drop(columns=['annotations_0','annotations_0.task','metadata','annotations',\\\n",
    "                      'subject_data','metadata.source',\\\n",
    "                      'metadata.live_project','metadata.subject_dimensions','annotations_1.task','annotations_1.value'])\n",
    "\n",
    "\n",
    "# Save the processed DataFrame to different file formats for further use\n",
    "# 1. Save to a CSV file with \";\" as the delimiter\n",
    "# This format is useful for sharing with tools that require semicolon-separated values\n",
    "#df.to_csv('processed_data.csv', sep=';', index=False)\n",
    "# 2. Save to a Parquet file\n",
    "# Parquet is a columnar storage format optimized for performance and storage efficiency\n",
    "#df.to_parquet('processed_data.parquet', index=False)\n",
    "# 3. Save to an Excel file\n",
    "\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print('writing to excel files')\n",
    "df_no_clouds = df[(df['annotations_0.value'] == 'No')]\n",
    "print(f'No clouds file shape: {df_no_clouds.shape}')\n",
    "df_no_clouds.to_excel('C:/knime/no_clouds.xlsx', index=False)\n",
    "\n",
    "df_broken_images = df[(df['annotations_0.value'] == 'The image is too broken up to see anything (bad data)')]\n",
    "print(f'No clouds file shape: {df_broken_images.shape}')\n",
    "df_broken_images.to_excel('C:/knime/broken_images.xlsx', index=False)\n",
    "df_cloudy_images = df[(df['annotations_0.value'].str.contains('Yes'))]\n",
    "print(f'No clouds file shape: {df_cloudy_images.shape}')\n",
    "#df_cloudy_images.to_excel('C:/knime/cloudy_images.xlsx', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcf123f0-ecd6-4011-8f74-334fa04a0b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Count by Rank:\n",
      "    rank  user_count\n",
      " student        3292\n",
      "advanced          38\n",
      "   elite           3\n"
     ]
    }
   ],
   "source": [
    "#some infotainment about the users\n",
    "\n",
    "deduped_df = df_cloudy_images.drop_duplicates(subset=['user_name'], keep='first')\n",
    "\n",
    "# 2. Group by rank and count users\n",
    "rank_counts = (\n",
    "    deduped_df\n",
    "    .groupby('rank', observed=True)  # observed=True handles categorical data efficiently\n",
    "    .size()\n",
    "    .reset_index(name='user_count')\n",
    ")\n",
    "\n",
    "# 3. Sort by weight (optional)\n",
    "rank_counts = rank_counts.sort_values('rank', key=lambda x: x.map({'student':1, 'advanced':2, 'elite':3}))\n",
    "\n",
    "# Show results\n",
    "print(\"\\nUser Count by Rank:\")\n",
    "print(rank_counts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d71f88-e427-4db7-843f-b62c8db1bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
